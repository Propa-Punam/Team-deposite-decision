# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y0QJXplfGS233cwmvc0VubezWBIDygZK
"""

import numpy as np

def sigmoid(z):
    return 1.0 / (1 + np.exp(-z))

def network_architecture(X, Y):
    n_x = X.shape[0]
    n_h = 10
    n_y = Y.shape[0]
    return (n_x, n_h, n_y)

def define_network_parameters(n_x, n_h, n_y):
    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_h, n_h) * 0.01
    b2 = np.zeros((n_h, 1))
    W3 = np.random.randn(n_y, n_h) * 0.01
    b3 = np.zeros((n_y, 1))
    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2, "W3": W3, "b3": b3}

def forward_propagation(X, params):
    Z1 = np.dot(params['W1'], X) + params['b1']
    A1 = sigmoid(Z1)
    Z2 = np.dot(params['W2'], A1) + params['b2']
    A2 = sigmoid(Z2)
    Z3 = np.dot(params['W3'], A2) + params['b3']
    A3 = sigmoid(Z3)
    return {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2, "Z3": Z3, "A3": A3}

def compute_error(predicted, actual):
    logprobs = np.multiply(np.log(predicted), actual) + np.multiply(np.log(1 - predicted), 1 - actual)
    cost = -np.sum(logprobs) / actual.shape[1]
    return np.squeeze(cost)

def backward_propagation(params, activations, X, Y):
    m = X.shape[1]
    dZ3 = activations['A3'] - Y
    dW3 = np.dot(dZ3, activations['A2'].T) / m
    db3 = np.sum(dZ3, axis=1, keepdims=True) / m
    dZ2 = np.dot(params['W3'].T, dZ3) * (1 - np.power(activations['A2'], 2))
    dW2 = np.dot(dZ2, activations['A1'].T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True) / m
    dZ1 = np.dot(params['W2'].T, dZ2) * (1 - np.power(activations['A1'], 2))
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m
    return {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2, "dW3": dW3, "db3": db3}

def update_parameters(params, derivatives, alpha=1.2):
    params['W1'] = params['W1'] - alpha * derivatives['dW1']
    params['b1'] = params['b1'] - alpha * derivatives['db1']
    params['W2'] = params['W2'] - alpha * derivatives['dW2']
    params['b2'] = params['b2'] - alpha * derivatives['db2']
    params['W3'] = params['W3'] - alpha * derivatives['dW3']
    params['b3'] = params['b3'] - alpha * derivatives['db3']
    return params

def neural_network(X, Y, n_h, num_iterations=100):
    n_x, n_y = X.shape[0], Y.shape[0]
    params = define_network_parameters(n_x, n_h, n_y)
    for i in range(num_iterations):
        results = forward_propagation(X, params)
        error = compute_error(results['A3'], Y)
        derivatives = backward_propagation(params, results, X, Y)
        params = update_parameters(params, derivatives)
    return params